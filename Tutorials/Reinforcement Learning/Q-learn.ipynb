{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19d4021",
   "metadata": {},
   "source": [
    "# Blackjack using Q-learning\n",
    "\n",
    "Now I want to use the original Q-learning algorithm to learn the optimal policy to play blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbdf3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c8bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearnAgent:\n",
    "    def __init__(self, lr, discount_factor, eps):\n",
    "        self.state = None\n",
    "        self.qtable = collections.defaultdict(float)\n",
    "        self.alpha = lr\n",
    "        self.gamma = discount_factor\n",
    "        self.eps = eps\n",
    "\n",
    "        self.action_space = [0, 1]\n",
    "        self.frozen = False\n",
    "    \n",
    "\n",
    "    def freeze(self, b = True):\n",
    "        self.frozen = b\n",
    "\n",
    "    def reset(self, state):\n",
    "        self.state = state\n",
    "    \n",
    "    def select_action(self):\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            # pick random action\n",
    "            return random.choice(self.action_space)\n",
    "        else:\n",
    "            #pick best action in current state\n",
    "            return self.get_best_action(self.state)\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        qs = [self.qtable[(state, action)] for action in self.action_space]\n",
    "        action = qs.index(max(qs))\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def update_q_table(self, reward, new_state, action):\n",
    "        if not self.frozen:\n",
    "            best_action = self.get_best_action(new_state)\n",
    "            self.qtable[(self.state, action)] += self.alpha * (reward + self.gamma * self.qtable[(new_state, best_action)] - self.qtable[self.state, action])\n",
    "\n",
    "    def step(self):\n",
    "        action = self.select_action()\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        if not self.frozen:\n",
    "            self.update_q_table(reward, observation, action)\n",
    "        self.state = observation\n",
    "        return terminated, truncated, reward\n",
    "    \n",
    "    def play_game(self):\n",
    "        state, _ = env.reset()\n",
    "        self.reset(state)\n",
    "        terminated, truncated = False, False\n",
    "        while not terminated and not truncated:\n",
    "            terminated, truncated, reward = self.step()\n",
    "        if terminated or truncated:\n",
    "            if reward > 0:\n",
    "                return 'won'\n",
    "            elif reward < 0:\n",
    "                return 'lost'\n",
    "            else:\n",
    "                return 'draw'\n",
    "\n",
    "q_learn_agent = QLearnAgent(lr = 0.1, discount_factor=0.1, eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f415a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_rate(agent):\n",
    "    agent.freeze()\n",
    "    outcomes = []\n",
    "    for _ in range(100000):\n",
    "        outcomes.append(agent.play_game())\n",
    "    return 100 * sum([1 if outcomes[i] == 'won' else 0 for i in range(len(outcomes))]) / len(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f78733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent BEFORE training has a winrate of 38.485%\n"
     ]
    }
   ],
   "source": [
    "print(f'The agent BEFORE training has a winrate of {get_win_rate(q_learn_agent)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0752b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [01:05<00:00, 30750.16it/s]\n"
     ]
    }
   ],
   "source": [
    "q_learn_agent.freeze(False)\n",
    "for _ in tqdm(range(2000000)):\n",
    "    q_learn_agent.play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25039f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent AFTER training has a winrate of 41.498%\n"
     ]
    }
   ],
   "source": [
    "print(f'The agent AFTER training has a winrate of {get_win_rate(q_learn_agent)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe9ef9",
   "metadata": {},
   "source": [
    "# Simple Agent\n",
    "\n",
    "I am going to implement a very simple rule-based strategy to see how it performs in comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b903f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedAgent:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "    \n",
    "    def freeze(self):\n",
    "        return 0\n",
    "\n",
    "    def reset(self, state):\n",
    "        self.state = state\n",
    "    \n",
    "    def select_action(self):\n",
    "        player_sum, dealer_card, usable_ace = self.state\n",
    "        # Always hit if under 12\n",
    "        if player_sum <= 11:\n",
    "            return 1  # hit\n",
    "        # Stand on 17 or higher\n",
    "        if player_sum >= 17:\n",
    "            return 0  # stick\n",
    "        # Between 12 and 16\n",
    "        if 12 <= player_sum <= 16:\n",
    "            if dealer_card >= 7:\n",
    "                return 1  # hit\n",
    "            else:\n",
    "                return 0  # stick\n",
    "        return 0\n",
    "    \n",
    "    def step(self):\n",
    "        action = self.select_action()\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        self.state = observation\n",
    "        return terminated, truncated, reward\n",
    "    \n",
    "    def play_game(self):\n",
    "        state, _ = env.reset()\n",
    "        self.reset(state)\n",
    "        terminated, truncated = False, False\n",
    "        while not terminated and not truncated:\n",
    "            terminated, truncated, reward = self.step()\n",
    "        if terminated or truncated:\n",
    "            if reward > 0:\n",
    "                return 'won'\n",
    "            elif reward < 0:\n",
    "                return 'lost'\n",
    "            else:\n",
    "                return 'draw'\n",
    "\n",
    "rule_based_agent = RuleBasedAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69ae8adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The simple agent has a winrate of 42.51%\n"
     ]
    }
   ],
   "source": [
    "print(f'The simple agent has a winrate of {get_win_rate(rule_based_agent)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a66c06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
