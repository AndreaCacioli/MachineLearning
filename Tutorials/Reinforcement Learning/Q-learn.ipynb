{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19d4021",
   "metadata": {},
   "source": [
    "# Blackjack using Q-learning\n",
    "\n",
    "Now I want to use the original Q-learning algorithm to learn the optimal policy to play blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afbdf3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, lr, discount_factor, eps):\n",
    "        self.state = None\n",
    "        self.qtable = collections.defaultdict(float)\n",
    "        self.alpha = lr\n",
    "        self.gamma = discount_factor\n",
    "        self.eps = eps\n",
    "\n",
    "        self.action_space = [0, 1]\n",
    "        self.frozen = False\n",
    "    \n",
    "\n",
    "    def freeze(self, b = True):\n",
    "        self.frozen = b\n",
    "\n",
    "    def reset(self, state):\n",
    "        self.state = state\n",
    "    \n",
    "    def select_action(self):\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            # pick random action\n",
    "            return random.choice(self.action_space)\n",
    "        else:\n",
    "            #pick best action in current state\n",
    "            return self.get_best_action(self.state)\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        qs = [self.qtable[(state, action)] for action in self.action_space]\n",
    "        action = qs.index(max(qs))\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def update_q_table(self, reward, new_state, action):\n",
    "        if not self.frozen:\n",
    "            best_action = self.get_best_action(new_state)\n",
    "            self.qtable[(self.state, action)] += self.alpha * (reward + self.gamma * self.qtable[(new_state, best_action)] - self.qtable[self.state, action])\n",
    "\n",
    "    def step(self):\n",
    "        action = self.select_action()\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        if not self.frozen:\n",
    "            self.update_q_table(reward, observation, action)\n",
    "        self.state = observation\n",
    "        return terminated, truncated, reward\n",
    "    \n",
    "    def play_game(self):\n",
    "        state, _ = env.reset()\n",
    "        self.reset(state)\n",
    "        terminated, truncated = False, False\n",
    "        while not terminated and not truncated:\n",
    "            terminated, truncated, reward = agent.step()\n",
    "        if terminated or truncated:\n",
    "            if reward > 0:\n",
    "                return 'won'\n",
    "            elif reward < 0:\n",
    "                return 'lost'\n",
    "            else:\n",
    "                return 'draw'\n",
    "\n",
    "agent = Agent(lr = 0.1, discount_factor=0.1, eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f415a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent BEFORE training has a winrate of 38.052%\n"
     ]
    }
   ],
   "source": [
    "def get_win_rate():\n",
    "    agent.freeze()\n",
    "    outcomes = []\n",
    "    for _ in range(100000):\n",
    "        outcomes.append(agent.play_game())\n",
    "    return 100 * sum([1 if outcomes[i] == 'won' else 0 for i in range(len(outcomes))]) / len(outcomes)\n",
    "\n",
    "print(f'The agent BEFORE training has a winrate of {get_win_rate()}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0752b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [01:06<00:00, 30284.06it/s]\n"
     ]
    }
   ],
   "source": [
    "agent.freeze(False)\n",
    "for _ in tqdm(range(2000000)):\n",
    "    agent.play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25039f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent AFTER training has a winrate of 41.775%\n"
     ]
    }
   ],
   "source": [
    "print(f'The agent AFTER training has a winrate of {get_win_rate()}%')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
